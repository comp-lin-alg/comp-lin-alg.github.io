
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3. Analysing algorithms &#8212; Computational linear algebra course 2020.0 documentation</title>
    <link rel="stylesheet" href="_static/fenics.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/proof.js"></script>
    <script async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. LU decomposition" href="L4_LU_decomposition.html" />
    <link rel="prev" title="2. QR Factorisation" href="L2_QR_factorisation.html" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

<link rel="stylesheet" href="_static/featured.css">


<link rel="shortcut icon" href="_static/icon.ico" />


  </head><body>
<div class="wrapper">
  <a href="index.html"><img src="_static/banner.png" width="900px" alt="Project Banner" /></a>
  <div id="access">
    <div class="menu">
      <ul>
          <li class="page_item"><a href="https://github.com/Computational-Linear-Algebra-Course/computational-linear-algebra-course" title="GitHub">GitHub</a></li>
      </ul>
    </div><!-- .menu -->
  </div><!-- #access -->
</div><!-- #wrapper -->


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="analysing-algorithms">
<h1><span class="section-number">3. </span>Analysing algorithms<a class="headerlink" href="#analysing-algorithms" title="Permalink to this headline">¶</a></h1>
<p>In the previous section we saw three algorithms to compute the QR
factorisation of a matrix. They have a beautiful mathematical
structure based on orthogonal projectors. But are they useful? To
answer this we need to know:</p>
<ol class="arabic simple">
<li><p>Is one faster than others?</p></li>
<li><p>Is one more sensitive than others to small perturbations due to
early truncation of the algorithm or due to round-off errors?</p></li>
</ol>
<p>In this course we will characterise answers to the first question by
operation count (acknowledging that this is an incomplete evaluation
of speed), and answers to the second question by analysing stability.</p>
<p>In this section we will discuss both of these questions by introducing
some general concepts but also looking at the examples of the QR
algorithms that we have seen so far.</p>
<div class="section" id="operation-count">
<h2><span class="section-number">3.1. </span>Operation count<a class="headerlink" href="#operation-count" title="Permalink to this headline">¶</a></h2>
<p>Operation count is one aspect of evaluating how long algorithms take.
Here we just note that this is not the only aspect, since transferring
data between different levels of memory on chips can be a serious (and
often dominant) consideration, even more so when we consider
algorithms that make use of large numbers of processors running in
parallel. However, operation count is what we shall focus on here.</p>
<p>In this course, a floating point operation (FLOP) will be any
arithmetic unary or binary operation acting on single numbers (such as
<span class="math notranslate nohighlight">\(+\)</span>, <span class="math notranslate nohighlight">\(-\)</span>, <span class="math notranslate nohighlight">\(\times\)</span>, <span class="math notranslate nohighlight">\(\div\)</span>, <span class="math notranslate nohighlight">\(\sqrt{}\)</span>). Of course, in reality, these
different operations have different relative costs, and codes can be
made more efficient by blending multiplications and additions (fused
multiply-adds) for example.  Here we shall simply apologise to
computer scientists in the class, and proceed with this
interpretation, since we are just making relative comparisons between
schemes. We shall also concentrate on asymptotic results in the limit
of large $n$ and/or $m$.</p>
</div>
<div class="section" id="operation-count-for-modified-gram-schmidt">
<h2><span class="section-number">3.2. </span>Operation count for modified Gram-Schmidt<a class="headerlink" href="#operation-count-for-modified-gram-schmidt" title="Permalink to this headline">¶</a></h2>
<p>We shall discuss operation counts through the example of the modified
Gram-Schmidt algorithm. We shall find that the operation count
is <span class="math notranslate nohighlight">\(\sim mn^2\)</span> to compute the QR factorisation, where the <span class="math notranslate nohighlight">\(\sim\)</span> symbol
means</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\lim_{m,n\to \infty}\frac{N_{\mbox{FLOPS}}}{2mn^2} = 1.\]</div>
</div></blockquote>
<p>To get this result, we return to the pseudocode for the modified Gram-Schmidt
algorithm, and concentrate on the operations that are happening
inside the inner <span class="math notranslate nohighlight">\(j\)</span> loop. Inside that loop there are two operations,</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(r_{ij} \gets q^*_iv_i\)</span>. This is the inner product of two vectors
in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>, which requires <span class="math notranslate nohighlight">\(m\)</span> multiplications and <span class="math notranslate nohighlight">\(m-1\)</span> additions,
so we count <span class="math notranslate nohighlight">\(2m-1\)</span> FLOPS per inner iteration.</p></li>
<li><p><span class="math notranslate nohighlight">\(v_j \gets v_j - r_{ij}q_i\)</span>. This requires <span class="math notranslate nohighlight">\(m\)</span> multiplications and <span class="math notranslate nohighlight">\(m\)</span>
subtractions, so we count <span class="math notranslate nohighlight">\(2m\)</span> FLOPS per inner iteration.</p></li>
</ol>
<p>At each iteration we require a combined operation count of <span class="math notranslate nohighlight">\(\sim 4m\)</span> FLOPS.
There are <span class="math notranslate nohighlight">\(n\)</span> outer iterations over <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(n-i-1\)</span> inner iterations
over <span class="math notranslate nohighlight">\(j\)</span>, which we can estimate by approximating the sum as an integral,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[N_{\mbox{FLOPS}} \sim \sum_{i=1}^n \sum_{j=i+1}^n 4m
\sim 4m \sum_{i=1}^n i\int_1^n x\,d x
\sim 4m\frac{n^2}{2} = 2mn^2,\]</div>
</div></blockquote>
<p>as suggested above.</p>
</div>
<div class="section" id="operation-count-for-householder">
<h2><span class="section-number">3.3. </span>Operation count for Householder<a class="headerlink" href="#operation-count-for-householder" title="Permalink to this headline">¶</a></h2>
<p>In the Householder algorithm, the computation is dominated by the
transformation</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[A_{k:m,k:n} \gets A_{k:m,k:n} -
\underbrace{2v_k\underbrace{(v_k^*A_{k:m,k:n})}_{1}}_{2},\]</div>
</div></blockquote>
<p>which must be done for each ‘k’ iteration. To evaluate the part marked
1 requires <span class="math notranslate nohighlight">\(n-k\)</span> inner products of vectors in <span class="math notranslate nohighlight">\(\mathbb{C}^{m-k}\)</span>, at a
total cost of <span class="math notranslate nohighlight">\(\sim 2(n-k)(m-k)\)</span> (we already examined inner products
in the previous example). To evaluate the part marked 2 then requires
the outer product of two vectors in <span class="math notranslate nohighlight">\(\mathbb{C}^{m-k}\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{C}^{n-k}\)</span> respectively, at a total cost of <span class="math notranslate nohighlight">\((m-k)(n-k)\)</span> FLOPs.
Finally two <span class="math notranslate nohighlight">\((k-m)\times(n-k)\)</span> matrices are substracted, at cost
<span class="math notranslate nohighlight">\((k-m)(n-k)\)</span>. Putting all this together gives <span class="math notranslate nohighlight">\(\sim 4(n-k)(m-k)\)</span> FLOPs
per <span class="math notranslate nohighlight">\(k\)</span> iteration.</p>
<p>Now we have to sum this over <span class="math notranslate nohighlight">\(k\)</span>, so the total operation count is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}4\sum_{k=1}^n(n-k)(m-k) = 4\sum_{k=1}^n(nm - k(n+m) + k^2)\\\sim 4n^2m - 4(n+m)\frac{n^2}{2} + 4\frac{n^3}{3}
= 2mn^2 - \frac{2n^3}{3}.\end{aligned}\end{align} \]</div>
</div></blockquote>
</div>
<div class="section" id="matrix-norms-for-discussing-stability">
<h2><span class="section-number">3.4. </span>Matrix norms for discussing stability<a class="headerlink" href="#matrix-norms-for-discussing-stability" title="Permalink to this headline">¶</a></h2>
<p>In the rest of this section we will discuss another important aspect
of analysing computational linear algebra algorithms, stability. To do
this we need to introduce some norms for matrices in addition to the
norms for vectors that we discussed in Section 1.</p>
<p>If we ignore their multiplication properties, matrices in
<span class="math notranslate nohighlight">\(\mathbb{C}^{m\times n}\)</span> can be added and scalar multiplied, hence we
can view them as a vector space, in which we can define norms, just
as we did for vectors.</p>
<p>One type of norm arises from simply treating the matrix entries as
entries of a vector and evaluating the 2-norm.</p>
<div class="proof proof-type-definition" id="id3">

    <div class="proof-title">
        <span class="proof-type">Definition 3.1</span>
        
            <span class="proof-title-name">(Frobenius norm)</span>
        
    </div><div class="proof-content">
<p>The Frobenius norm is the matrix version of the 2-norm, defined as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|A\|_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^nA_{ij}^2}.\]</div>
</div></blockquote>
</div></div><p>(Exercise: show that <span class="math notranslate nohighlight">\(\|AB\|_F \leq \|A\|_F\|B\|_F\)</span>.)</p>
<p>Another type of norm measures the maximum amount of stretching the matrix
can cause when multiplying a vector.</p>
<div class="proof proof-type-definition" id="id4">

    <div class="proof-title">
        <span class="proof-type">Definition 3.2</span>
        
            <span class="proof-title-name">(Induced matrix norm)</span>
        
    </div><div class="proof-content">
<p>Given an <span class="math notranslate nohighlight">\(m\times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> and any chosen vector norms
<span class="math notranslate nohighlight">\(\|\cdot\|_{(n)}\)</span> and <span class="math notranslate nohighlight">\(\|\cdot\|_{(m)}\)</span> on <span class="math notranslate nohighlight">\(\mathbb{C}^n\)</span> and
<span class="math notranslate nohighlight">\(\mathbb{C}^m\)</span>, respectively, the induced norm on <span class="math notranslate nohighlight">\(A\)</span> is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|A\|_{(m,n)} = \sup_{x\in\mathbb{C}^n, x\neq 0}\frac{\|Ax\|_{(m)}}
{\|x\|_{(n)}}.\]</div>
</div></blockquote>
</div></div><p>Directly from the definition we can show</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|Ax\|_{(m)}}{\|x\|_{(n)}} \leq \sup_{x\in\mathbb{C}^n, x\neq 0}
\frac{\|Ax\|_{(m)}}
{\|x\|_{(n)} = \|A\|_{(m,n)}},\]</div>
</div></blockquote>
<p>and hence <span class="math notranslate nohighlight">\(\|Ax\|\leq \|A\|\|x\|\)</span> whenever we use an induced matrix norm.</p>
</div>
<div class="section" id="norm-inequalities">
<h2><span class="section-number">3.5. </span>Norm inequalities<a class="headerlink" href="#norm-inequalities" title="Permalink to this headline">¶</a></h2>
<p>Often it is difficult to find exact values for norms, so we compute upper
bounds using inequalities instead. Here are a few useful inequalities.</p>
<div class="proof proof-type-definition" id="id5">

    <div class="proof-title">
        <span class="proof-type">Definition 3.3</span>
        
            <span class="proof-title-name">(H&quot;older inequality)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x,y\in \mathbb{C}^m\)</span>, and <span class="math notranslate nohighlight">\(p,q \in \mathbb{R}+\)</span> such that
<span class="math notranslate nohighlight">\(\frac{1}{p}+{1}{q} = 1\)</span>. Then</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[|x^*y| \leq \|x\|_p\|y\|_q.\]</div>
</div></blockquote>
</div></div><p>In the case <span class="math notranslate nohighlight">\(p=q=2\)</span> this becomes the Cauchy-Schwartz inequality.</p>
<div class="proof proof-type-definition" id="id6">

    <div class="proof-title">
        <span class="proof-type">Definition 3.4</span>
        
            <span class="proof-title-name">(Cauchy-Schwartz inequality)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x,y\in \mathbb{C}^m\)</span>. Then</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[|x^*y| \leq \|x\|_2\|y\|_2.\]</div>
</div></blockquote>
</div></div><p>For example, we can use this to bound the operator norm of the outer
product <span class="math notranslate nohighlight">\(A=uv^*\)</span> of two vectors.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|Ax\|_2 = \|uv^*x\|_2 = \|u(v^*x)\|_2 = |v^*x|\|u\|_2
\leq \|u\|_2\|v\|_2\|x\|_2,\]</div>
</div></blockquote>
<p>so <span class="math notranslate nohighlight">\(\|A\|_2 \leq \|u\|_2\|v\|_2\)</span>.</p>
<p>We can also compute bounds for <span class="math notranslate nohighlight">\(\|AB\|_2\)</span>.</p>
<div class="proof proof-type-theorem" id="id7">

    <div class="proof-title">
        <span class="proof-type">Theorem 3.5</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(A\in \mathbb{C}^{l\times m}\)</span>, <span class="math notranslate nohighlight">\(B\in \mathbb{C}^{m\times n}\)</span>. Then</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|AB|_{(l,n)} \leq \|A\|_{(l,m)}\|B\|_{(m,n)}.\]</div>
</div></blockquote>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|ABx\|_{(l)} \leq \|A\|_{(l,m)}\|Bx\|_{(m)}
\leq \|A\|_{(l,m)}\|B\|_{(m,n)}\|x\|_{(n)},\]</div>
</div></blockquote>
<p>so</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|AB\|_{(l,n)} = \sup_{x\neq 0}\frac{\|ABx\|_{(l)}}{\|x\|_{(n)}}
\leq \|A\|_{(l,m)}\|B\|_{(m,n)},\]</div>
</div></blockquote>
<p>as required.</p>
</div></div></div>
<div class="section" id="condition-number">
<h2><span class="section-number">3.6. </span>Condition number<a class="headerlink" href="#condition-number" title="Permalink to this headline">¶</a></h2>
<p>The key tool to understanding numerical stability of computational
linear algebra algorithms is the condition number.  The condition
number is a very general concept that measures the behaviour of a
mathematical problem under perturbations. Here we think of a
mathematical problem as a function <span class="math notranslate nohighlight">\(f:X\to Y\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are
normed vector spaces (further generalisations are possible). It is
often the case that <span class="math notranslate nohighlight">\(f\)</span> has different properties under perturbation
for different values of <span class="math notranslate nohighlight">\(x\in X\)</span>.</p>
<div class="proof proof-type-definition" id="id8">

    <div class="proof-title">
        <span class="proof-type">Definition 3.6</span>
        
            <span class="proof-title-name">(Well conditioned and ill conditioned.)</span>
        
    </div><div class="proof-content">
<p>We say that a problem is well conditioned (at <span class="math notranslate nohighlight">\(x\)</span>) if small changes
in <span class="math notranslate nohighlight">\(x\)</span> lead to small changes in <span class="math notranslate nohighlight">\(f(x)\)</span>. We say that a problem is
ill conditioned if small changes in <span class="math notranslate nohighlight">\(x\)</span> lead to large changes in
<span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
</div></div><p>These changes are measured by the condition number.</p>
<div class="proof proof-type-definition" id="id9">

    <div class="proof-title">
        <span class="proof-type">Definition 3.7</span>
        
            <span class="proof-title-name">(Absolute condition number.)</span>
        
    </div><div class="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\delta x\)</span> be a perturbation so that <span class="math notranslate nohighlight">\(x\mapsto x + \delta x\)</span>.
The corresponding change in <span class="math notranslate nohighlight">\(f(x)\)</span> is <span class="math notranslate nohighlight">\(\delta f(x)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\delta f(x) = f(x + \delta x) - f(x).\]</div>
<p>The absolute condition number of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[\hat{\kappa} = \sup_{\delta x \neq 0}\frac{\|\delta f\|}{\|\delta x\|},\]</div>
<p>i.e. the maximum that <span class="math notranslate nohighlight">\(f\)</span> can change relative to the size of the
perturbation <span class="math notranslate nohighlight">\(\delta x\)</span>.</p>
<p>It is easier to consider linearised perturbations, defining
a Jacobian matrix <span class="math notranslate nohighlight">\(J(x)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[J(x)\delta x = \lim_{\epsilon \to 0}
\frac{f(x+\epsilon\delta x)-f(x)}{\epsilon}\]</div>
<p>and then the linear absolute condition number is</p>
<div class="math notranslate nohighlight">
\[\hat{\kappa} = \sup_{\delta x \neq 0}\frac{\|J(x)\delta x\|}
{\|\delta x\|} = \|J(x)\|,\]</div>
<p>which is the operator norm of <span class="math notranslate nohighlight">\(J(x)\)</span>.</p>
</div></div><p>This definition could be improved by measuring this change relative to the
size of <span class="math notranslate nohighlight">\(f\)</span> itself.</p>
<div class="proof proof-type-definition" id="id10">

    <div class="proof-title">
        <span class="proof-type">Definition 3.8</span>
        
            <span class="proof-title-name">(Relative condition number.)</span>
        
    </div><div class="proof-content">
<p>The relative condition number of a problem <span class="math notranslate nohighlight">\(f\)</span> measures the changes
<span class="math notranslate nohighlight">\(\delta x\)</span> and <span class="math notranslate nohighlight">\(\delta f\)</span> relative to the sizes of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="math notranslate nohighlight">
\[\kappa = \sup_{\delta \neq 0}\frac{\|\delta f\|/\|f\|}
{\|\delta x\|/\|x\|}.\]</div>
<p>The linear relative condition number is</p>
<div class="math notranslate nohighlight">
\[\kappa = \frac{\|J\|/\|f\|}{\|x\|} = \frac{\|J\|\|x\|}{\|f\|}.\]</div>
</div></div><p>Since we use floating point numbers on computers, it makes more sense
to consider relative condition numbers in computational linear
algebra, and from here on we will always use them whenever we mention
condition numbers. If <span class="math notranslate nohighlight">\(\kappa\)</span> is small (<span class="math notranslate nohighlight">\(1-100\)</span>, say) then we say that
a problem is well conditioned. If <span class="math notranslate nohighlight">\(\kappa\)</span> is large (<span class="math notranslate nohighlight">\(&gt;10^6\)</span>, say),
then we say that a problem is ill conditioned.</p>
<p>As a first example, consider the problem of finding the square root,
<span class="math notranslate nohighlight">\(f:x\mapsto \sqrt{x}\)</span>, a one dimensional problem. In this case,
<span class="math notranslate nohighlight">\(J=x^{1/2}/2\)</span>. The (linear) condition number is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\kappa = \frac{|x^{-1/2}/2||x|}{|x^{1/2}|}=1/2.\]</div>
</div></blockquote>
<p>Hence, the problem is well-conditioned.</p>
<p>As a second example, consider the problem of finding the roots of a
polynomial, given its coefficients. Specifically, we consider the
polynomial <span class="math notranslate nohighlight">\(x^2 - 2x +1 = (x-1)^2\)</span>, which has two roots equal
to 1. Here we consider the change in roots relative to the coefficient
of <span class="math notranslate nohighlight">\(x^0\)</span> (which is 1). Making a small perturbation to the polynomial,
<span class="math notranslate nohighlight">\(x^2 - 2x + 0.9999 = (x-0.99)(x-1.01)\)</span>, so a relative change of <span class="math notranslate nohighlight">\(10^{-4}\)</span>
gives a relative change of <span class="math notranslate nohighlight">\(10^{-2}\)</span> in the roots. Using the general formula</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[r = 1 \pm\sqrt{1-c} = 1 \pm \sqrt{\delta c} \implies
\delta r = \pm \sqrt{\delta c},\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(r\)</span> returns the two roots with perturbations <span class="math notranslate nohighlight">\(\delta r\)</span> and <span class="math notranslate nohighlight">\(c\)</span>
is the coefficient of <span class="math notranslate nohighlight">\(x^0\)</span> with perturbatino <span class="math notranslate nohighlight">\(\delta c\)</span>.
is the perturbation to the coefficient of <span class="math notranslate nohighlight">\(x^0\)</span> (so 1 becomes
<span class="math notranslate nohighlight">\(1+\delta c\)</span>). The (nonlinear) condition number is then
the sup over <span class="math notranslate nohighlight">\(\delta c\neq 0\)</span> of</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{|{\delta r}|/|r|}{|\delta c|/|c|}
= \frac{|{\delta r}|}{|\delta c|} = \frac{|\delta c|^{1/2}}{|\delta c|}
= |\delta c|^{-1/2} \to \infty \mbox{ as } \delta c \to 0,\]</div>
</div></blockquote>
<p>so the condition number is unbounded and the problem is
catastrophically ill conditioned. For an even more vivid example, see
the conditioning of the roots of the Wilkinson polynomial.</p>
</div>
<div class="section" id="conditioning-of-linear-algebra-computations">
<h2><span class="section-number">3.7. </span>Conditioning of linear algebra computations<a class="headerlink" href="#conditioning-of-linear-algebra-computations" title="Permalink to this headline">¶</a></h2>
<p>We now look at the condition number of problems from linear algebra.
The first problem we examine is the problem of matrix-vector
multiplication, i.e. for a fixed matrix <span class="math notranslate nohighlight">\(A\in \mathbb{C}^{m\times n}\)</span>,
the problem is to find <span class="math notranslate nohighlight">\(Ax\)</span> given <span class="math notranslate nohighlight">\(x\)</span>. The problem is linear,
with <span class="math notranslate nohighlight">\(J=A\)</span>, so the condition number is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\kappa = \frac{\|A\|\|x\|}{\|Ax\|}.\]</div>
</div></blockquote>
<p>When <span class="math notranslate nohighlight">\(A\)</span> is non singular, we can write <span class="math notranslate nohighlight">\(x = A^{-1}Ax\)</span>, and</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|x\| = \|A^{-1}Ax\| \leq \|A^{-1}\|\|Ax\|,\]</div>
</div></blockquote>
<p>so</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\kappa \leq \frac{\|A\|\|A^{-1}\|\|Ax\|}{\|Ax\|}
= \|A\|\|A^{-1}\|.\]</div>
</div></blockquote>
<p>We call this upper bound the condition number <span class="math notranslate nohighlight">\(\kappa(A)\)</span> of the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>The next problem we consider is the condition number of solving
<span class="math notranslate nohighlight">\(Ax=b\)</span>, with <span class="math notranslate nohighlight">\(b\)</span> fixed but considering perturbations to <span class="math notranslate nohighlight">\(A\)</span>. So, we
have <span class="math notranslate nohighlight">\(f:A\mapsto x\)</span>. The condition number of this problem measures how
small changes <span class="math notranslate nohighlight">\(\delta A\)</span> to <span class="math notranslate nohighlight">\(A\)</span> translate to changes <span class="math notranslate nohighlight">\(\delta x\)</span> to
<span class="math notranslate nohighlight">\(x\)</span>. The perturbed problem is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[(A + \delta A)(x + \delta x) = b,\]</div>
</div></blockquote>
<p>which simplifies (using <span class="math notranslate nohighlight">\(Ax=b\)</span>) to</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\delta A(x + \delta x) + A\delta x = 0,\]</div>
</div></blockquote>
<p>which is independent of <span class="math notranslate nohighlight">\(b\)</span>. If we are considering the linear
condition number, we can drop the nonlinear term, and we get</p>
<blockquote>
<div><blockquote>
<div><div class="math notranslate nohighlight">
\[\delta A x + A \delta x = 0, \implies \delta x = -A^{-1}\delta Ax,\]</div>
</div></blockquote>
<p>from which we may compute the bound</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\|\delta x\| \leq \|A^{-1}\|\|\delta A\|\|x\|.\]</div>
</div></blockquote>
</div></blockquote>
<p>Then, we can compute the condition number</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\kappa = \sup_{\|\delta A\|\neq 0}
\frac{\|\delta x\|/\|x\|}{\|\delta A\|/\|A\|}.
\leq \sup_{\|\delta A\|\neq 0}
\frac{\|A^{-1}\|\|\delta A\|\|x\|/\|x\|}{\|\delta A\|/\|A\|}.
= \|A^{-1}\|\|A\| = \kappa(A),\]</div>
</div></blockquote>
<p>having used the bound for <span class="math notranslate nohighlight">\(\delta x\)</span>. Hence the bound on the condition
number for this problem is the condition number of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div>
<div class="section" id="floating-point-numbers-and-arithmetic">
<h2><span class="section-number">3.8. </span>Floating point numbers and arithmetic<a class="headerlink" href="#floating-point-numbers-and-arithmetic" title="Permalink to this headline">¶</a></h2>
<p>Floating point number systems on computers use a discrete and finite
representation of the real numbers. One of the first things we can
deduce from this fact is that there exists a largest and a smallest
positive number.  In “double precision”, the standard floating point
number format for scientific computing these days, the largest number
is <span class="math notranslate nohighlight">\(N_{\max}\approx 1.79\times 10^{308}\)</span>, and the smallest number is
<span class="math notranslate nohighlight">\(N_{\min}\approx 2.23 \times 10^{-308}\)</span>. The second thing that we can
deduce is that there must be gaps between adjacent numbers in the
number system. In the double precision format, the interval <span class="math notranslate nohighlight">\([1,2]\)</span> is
subdivided as <span class="math notranslate nohighlight">\((1,1+2^{-52},1+2\times 2^{-52},1+3\times 2^{-52},
\ldots, 2)\)</span>. The next interval <span class="math notranslate nohighlight">\([2,4]\)</span> is subdivided as <span class="math notranslate nohighlight">\((2, 2 +
2^{-51}, 2 + 2\times 2^{-51}, \ldots, 4)\)</span>.  In general, the interval
<span class="math notranslate nohighlight">\([2^j, 2^{j+1}]\)</span> is subdivided by multiplying the set subdividing
<span class="math notranslate nohighlight">\([1,2]\)</span> by <span class="math notranslate nohighlight">\(2^j\)</span>. In this representation, the gaps between numbers
scale with the number size. We call this set of numbers the (double
precision) floating point numbers <span class="math notranslate nohighlight">\(\mathbb{F}\subset \mathbb{R}\)</span>.</p>
<p>A key aspect of a floating point number system is “machine epsilon”
(<span class="math notranslate nohighlight">\(\varepsilon\)</span>), which measures the larges relative distance between
two numbers. Considering the description above, we see that
<span class="math notranslate nohighlight">\(\varepsilon\)</span> is the the distance between 1 and the adjacent number, i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\varepsilon = 2^{-53} \approx 1.11 \times 10^{-16}.\]</div>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(\varepsilon\)</span> defines the accuracy with which arbitrary real numbers
(within the range of the maximum magnitude above) can be approximated
in <span class="math notranslate nohighlight">\(\mathbb{F}\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\forall x \in \mathbb{R}, \, \exists x'\in \mathbb{F}
\mbox{ such that } |x-x'| \leq \varepsilon |x|.\]</div>
</div></blockquote>
<div class="proof proof-type-definition" id="id11">

    <div class="proof-title">
        <span class="proof-type">Definition 3.9</span>
        
            <span class="proof-title-name">(Floating point rounding function)</span>
        
    </div><div class="proof-content">
<p>We define <span class="math notranslate nohighlight">\(f_L:\mathbb{R}\to \mathbb{F}\)</span> as the function that rounds
<span class="math notranslate nohighlight">\(x\in \mathbb{R}\)</span> to the nearest floating point number.</p>
</div></div><p>The following axiom is just a formal presentation of the properties
of floating point numbers that we discussed below.</p>
<div class="proof proof-type-definition" id="id12">

    <div class="proof-title">
        <span class="proof-type">Definition 3.10</span>
        
            <span class="proof-title-name">(Floating point axiom I)</span>
        
    </div><div class="proof-content">
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\forall x \in \mathbb{R}, \, \exists \epsilon' \mbox{ with }
|\epsilon'| \leq \varepsilon,\\\mbox{ such that } f_L(x) = x(1+\epsilon').\end{aligned}\end{align} \]</div>
</div></div><p>The arithmetic operations <span class="math notranslate nohighlight">\(+,-,\times,\div\)</span> on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> have
analogous operations <span class="math notranslate nohighlight">\(\oplus,\ominus,\otimes\)</span>, etc. In general, binary
operators <span class="math notranslate nohighlight">\(\odot\)</span> (as a general symbol representing the floating point
version of a real arithmetic operator <span class="math notranslate nohighlight">\(\cdot\)</span> which could be any of the
above) are constructed such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[x\odot y = f_L(x\cdot y),\]</div>
</div></blockquote>
<p>for <span class="math notranslate nohighlight">\(x,y\in \mathbb{F}\)</span>, with <span class="math notranslate nohighlight">\(\cdot\)</span> being one of <span class="math notranslate nohighlight">\(+,-,\times,\div\)</span>.</p>
<div class="proof proof-type-definition" id="id13">

    <div class="proof-title">
        <span class="proof-type">Definition 3.11</span>
        
            <span class="proof-title-name">(Floating point axiom II)</span>
        
    </div><div class="proof-content">
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\forall x,y \in \mathbb{F}, \exists \epsilon' \mbox{ with }
|\epsilon'|\leq \varepsilon,\mbox{ such that }\\x\odot y = (x\cdot y)(1 + \epsilon').\end{aligned}\end{align} \]</div>
</div></div></div>
<div class="section" id="stability">
<h2><span class="section-number">3.9. </span>Stability<a class="headerlink" href="#stability" title="Permalink to this headline">¶</a></h2>
<p>Stability describes the perturbation behaviour of a numerical algorithm
when used to solve a problem on a computer. Now we have two problems
<span class="math notranslate nohighlight">\(f:X\to Y\)</span> (the original problem implemented in the real numbers), and
<span class="math notranslate nohighlight">\(\tilde{f}:X\to Y\)</span> (the modified problem where floating point numbers
are used at each step).</p>
<p>Given a problem <span class="math notranslate nohighlight">\(f\)</span> (such as computing the QR factorisation), we are given:</p>
<ol class="arabic simple">
<li><p>A floating point system <span class="math notranslate nohighlight">\(\mathbb{F}\)</span>,</p></li>
<li><p>An algorithm for computing <span class="math notranslate nohighlight">\(f\)</span>,</p></li>
<li><p>A floating point implementation <span class="math notranslate nohighlight">\(\tilde{f}\)</span> for <span class="math notranslate nohighlight">\(f\)</span>.</p></li>
</ol>
<p>Then the chosen <span class="math notranslate nohighlight">\(x\in X\)</span> is rounded to <span class="math notranslate nohighlight">\(x'=f_L(x)\)</span>, and supplied to
the floating point implementation of the algorithm to obtain
<span class="math notranslate nohighlight">\(\tilde{f}(x)\in Y\)</span>.</p>
<p>Now we want to compare <span class="math notranslate nohighlight">\(f(x)\)</span> with <span class="math notranslate nohighlight">\(\tilde{f}(x)\)</span>. We can measure the
absolute error</p>
<blockquote>
<div><blockquote>
<div><div class="math notranslate nohighlight">
\[\|\tilde{f}(x)-f(x)\|,\]</div>
</div></blockquote>
<p>or the relative error (taking into account the size of <span class="math notranslate nohighlight">\(f\)</span>),</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{f}(x)-f(x)\|}{\|f(x)\|}.\]</div>
</div></blockquote>
</div></blockquote>
<p>An aspiration (but an unrealistic one) would be to aim for an algorithm
to accurate to machine precision, i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{f}(x)-f(x)\|}{\|f(x)\|} = \mathcal{O}(\varepsilon),\]</div>
</div></blockquote>
<p>by which we mean that <span class="math notranslate nohighlight">\(\exists C&gt;0\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{f}(x)-f(x)\|}{\|f(x)\|} \leq C\varepsilon,\]</div>
</div></blockquote>
<p>for sufficiently small <span class="math notranslate nohighlight">\(\varepsilon\)</span>.</p>
<div class="proof proof-type-definition" id="id14">

    <div class="proof-title">
        <span class="proof-type">Definition 3.12</span>
        
            <span class="proof-title-name">(Stability)</span>
        
    </div><div class="proof-content">
<p>An algorithm <span class="math notranslate nohighlight">\(\tilde{f}\)</span> for <span class="math notranslate nohighlight">\(f\)</span> is stable if for each <span class="math notranslate nohighlight">\(x\in X\)</span>,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{f}(x)-f(\tilde{x})\|}{\|f(\tilde{x})\|} = \mathcal{O}(\varepsilon),\]</div>
</div></blockquote>
<p>for all <span class="math notranslate nohighlight">\(\tilde{x}\)</span> with</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{x}-x\|}{\|x\|} = \mathcal{O}(\varepsilon).\]</div>
</div></blockquote>
</div></div><p>We say that a stable algorithm gives nearly the right answer to nearly the
right question.</p>
<div class="proof proof-type-definition" id="id15">

    <div class="proof-title">
        <span class="proof-type">Definition 3.13</span>
        
            <span class="proof-title-name">(Backward stability)</span>
        
    </div><div class="proof-content">
<p>An algorithm <span class="math notranslate nohighlight">\(\tilde{f}\)</span> for <span class="math notranslate nohighlight">\(f\)</span> is backward stable if for each <span class="math notranslate nohighlight">\(x\in X\)</span>,
<span class="math notranslate nohighlight">\(\exists\tilde{x}\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\tilde{f}(x) = f(\tilde{x}),
\mbox{ with }
\frac{\|\tilde{x}-x\|}{\|x\|} = \mathcal{O}(\varepsilon).\]</div>
</div></blockquote>
</div></div><p>A backward stable algorithm gives exactly the right answer to nearly
the right answer. The following result shows what accuracy we can expect
from a backward stable algorithm, which involves the condition number
of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="proof proof-type-theorem" id="id16">
<span id="accuracy-backward"></span>
    <div class="proof-title">
        <span class="proof-type">Theorem 3.14</span>
        
            <span class="proof-title-name">(Accuracy of a backward stable algorithm)</span>
        
    </div><div class="proof-content">
<p>Suppose that a backward stable algorithm is applied to solve problem
<span class="math notranslate nohighlight">\(f:X\to Y\)</span> with condition number <span class="math notranslate nohighlight">\(\kappa\)</span> using a floating point
number system satisfying the floating point axioms I and II. Then
the relative error satisfies</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{f}(x) - f(x)\|}{\|f(x)\|}
= \mathcal{O}(\kappa(x)\epsilon).\]</div>
</div></blockquote>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>Since <span class="math notranslate nohighlight">\(\tilde{f}\)</span> is backward stable, we have <span class="math notranslate nohighlight">\(\tilde{x}\)</span> with
<span class="math notranslate nohighlight">\(\tilde{f}(x)=f(\tilde{x})\)</span> and <span class="math notranslate nohighlight">\(\|\tilde{x}-x\|/\|x\| =
\mathcal{O}(\varepsilon)\)</span> as above.
Then,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\|\tilde{f}(x)-f(x)\|}{\|f(x)\|} =
\frac{\|f(\tilde{x})-f(x)\|}{\|f(x)\|},\\=       \underbrace{\frac{\|f(\tilde{x})-f(x)\|}{\|f(x)\|}
\frac{\|x\|}{\|\tilde{x}-x\|}}_{=\kappa}
\underbrace{\frac{\|\tilde{x}-x\|}{\|x\|}}_{=\mathcal{O}(\epsilon)},\end{aligned}\end{align} \]</div>
</div></blockquote>
<p>as required.</p>
</div></div><p>This type of calculation is known as backward error analysis,
originally introduced by Jim Wilkinson to analyse the accuracy of
eigenvalue calculations using the PILOT ACE, one of the early
computers build at the National Physical Laboratory in the late 1940s
and early 1950s. In backward error analysis we investigate the
accuracy via conditioning and stability. This is usually much easier
than forward analysis, where one would simply try to keep a running
tally of errors committed during each step of the algorithm.</p>
</div>
<div class="section" id="backward-stability-of-the-householder-algorithm">
<h2><span class="section-number">3.10. </span>Backward stability of the Householder algorithm<a class="headerlink" href="#backward-stability-of-the-householder-algorithm" title="Permalink to this headline">¶</a></h2>
<p>We now consider the example of the problem of finding the QR
factorisation of a matrix <span class="math notranslate nohighlight">\(A\)</span>, implemented in floating point
arithmetic using the Householder method. The input is <span class="math notranslate nohighlight">\(A\)</span>, and the
exact output is <span class="math notranslate nohighlight">\(Q,R\)</span>, whilst the floating point algorithm output is
<span class="math notranslate nohighlight">\(\tilde{Q},\tilde{R}\)</span>. Here, we consider <span class="math notranslate nohighlight">\(\tilde{Q}\)</span> as the exact
unitary matrix produced by composing Householder rotations made by
the floating point vectors <span class="math notranslate nohighlight">\(\tilde{v}_k\)</span> that approximate the <span class="math notranslate nohighlight">\(v_k\)</span>
vectors in the exact arithmetic Householder algorithm.</p>
<p>For this problem, backwards stability means
that there exists a perturbed input <span class="math notranslate nohighlight">\(A+\delta A\)</span>, with <span class="math notranslate nohighlight">\(\|\delta
A\|/\|A\| =\mathcal{O}(\varepsilon)\)</span>, such that <span class="math notranslate nohighlight">\(\tilde{Q},\tilde{R}\)</span>
are exact solutions to the problem, i.e. <span class="math notranslate nohighlight">\(\tilde{Q}\tilde{R}=A+\delta
A\)</span>. This means that there is very small backward error,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|A-\tilde{Q}\tilde{R}\|}{\|A\|} = \mathcal{O}(\varepsilon).\]</div>
</div></blockquote>
<p>It turns out that the Householder method is backwards stable.</p>
<div class="proof proof-type-theorem" id="id17">

    <div class="proof-title">
        <span class="proof-type">Theorem 3.15</span>
        
    </div><div class="proof-content">
<p>Let the QR factorisation be computed for <span class="math notranslate nohighlight">\(A\)</span> using a floating point
implementation of the Householder algorithm. This factorisation is
backwards stable, i.e. the result <span class="math notranslate nohighlight">\(\tilde{Q}\tilde{R}\)</span> satisfy</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\tilde{Q}\tilde{R} = A + \delta A, \quad
\frac{\|\delta A\|}{\|A\|} = \mathcal{O}(\varepsilon).\]</div>
</div></blockquote>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>See the textbook by Trefethen and Bau, Lecture 16.</p>
</div></div></div>
<div class="section" id="backward-stability-for-solving-a-linear-system-using-qr">
<h2><span class="section-number">3.11. </span>Backward stability for solving a linear system using QR<a class="headerlink" href="#backward-stability-for-solving-a-linear-system-using-qr" title="Permalink to this headline">¶</a></h2>
<p>The QR factorisation provides a method for solving systems of
equations <span class="math notranslate nohighlight">\(Ax=b\)</span> for <span class="math notranslate nohighlight">\(x\)</span> given <span class="math notranslate nohighlight">\(b\)</span>, where <span class="math notranslate nohighlight">\(A\)</span> is an invertible
matrix. Substituting <span class="math notranslate nohighlight">\(A=QR\)</span> and then left-multiplying by <span class="math notranslate nohighlight">\(Q^*\)</span>
gives</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Rx = Q^*b = y.\]</div>
</div></blockquote>
<p>The solution of this equation is <span class="math notranslate nohighlight">\(x=R^{-1}y\)</span>, but if there is one
message to take home from this course, it is that you should <em>never</em>
form the inverse of a matrix. It is especially disasterous to use
Kramer’s rule, which has an operation count scaling like
<span class="math notranslate nohighlight">\(\mathcal{O}(m!)\)</span> and is numerically unstable. There are some better
algorithms for finding the inverse of a matrix if you really need it,
but in almost every situation it is better to <em>solve</em> a matrix system
rather than forming the inverse of the matrix and multiplying it.  It
is particularly easy to solve an equation formed from an upper
triangular matrix.  Written in components, this equation is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}R_{11}x_1 + R_{12}x_2 + \ldots + R_{1(m-1)}x_{m-1} + R_{1m}x_m = y_1,\\0x_1 + R_{22}x_2 + \ldots + R_{2(m-1)}x_{m-1} + R_{2m}x_m = y_2,\\\vdots\\0x_1 + 0x_2 + \ldots + R_{(m-1)(m-1)}x_{m-1} + R_{(m-1)m}x_m = y_{m-1},\\ 0x_1 + 0x_2 + \ldots + 0x_{m-1} + R_{mm}x_m = y_{m}.\end{aligned}\end{align} \]</div>
</div></blockquote>
<p>The last equation yields <span class="math notranslate nohighlight">\(x_m\)</span> directly by dividing by <span class="math notranslate nohighlight">\(R_{mm}\)</span>, then
we can use this value to directly compute <span class="math notranslate nohighlight">\(x_{m-1}\)</span>. This is repeated
for all of the entries of <span class="math notranslate nohighlight">\(x\)</span> from <span class="math notranslate nohighlight">\(m\)</span> down to 1. This procedure is
called back substitution, which we summarise in the following
pseudo-code.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_m  \gets y_m/R_{mm}\)</span></p></li>
<li><p>FOR <span class="math notranslate nohighlight">\(i= m-1\)</span> TO 1 (BACKWARDS)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x_i \gets (y_i - \sum_{k=i+1}^mR_{ik}x_k)/R_{ii}\)</span></p></li>
</ul>
</li>
</ul>
<p>In each iteration, there are <span class="math notranslate nohighlight">\(m-i-1\)</span> multiplications and subtractions
plus a division, so the total operation count is <span class="math notranslate nohighlight">\(\sim m^2\)</span> FLOPs.</p>
<p>In comparison, the least bad way to form the inverse <span class="math notranslate nohighlight">\(Z\)</span> of <span class="math notranslate nohighlight">\(R\)</span> is to
write <span class="math notranslate nohighlight">\(RZ = I\)</span>. Then, the <a href="#id1"><span class="problematic" id="id2">`</span></a>k`th column of this equation is</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Rz_k = e_k,\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(z_k\)</span> is the kth column of <span class="math notranslate nohighlight">\(Z\)</span>. Solving for each column
independently using back substitution leads to an operation count of
<span class="math notranslate nohighlight">\(\sim m^3\)</span> FLOPs, much slower than applying back substitution directly
to <span class="math notranslate nohighlight">\(b\)</span>. Hopefully this should convince you to always seek an
alternative to forming the inverse of a matrix.</p>
<p>There are then three steps to solving <span class="math notranslate nohighlight">\(Ax=b\)</span> using QR factorisation.</p>
<ol class="arabic simple">
<li><p>Find the QR factorisation of <span class="math notranslate nohighlight">\(A\)</span> (here we shall use the Householder
algorithm).</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(y=Q^*b\)</span> (using the implicit multiplication algorithm).</p></li>
<li><p>Solve <span class="math notranslate nohighlight">\(Rx=y\)</span> (using back substitution).</p></li>
</ol>
<p>So our <span class="math notranslate nohighlight">\(f\)</span> here is the solution of <span class="math notranslate nohighlight">\(Ax=b\)</span> given <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(A\)</span>, and our
<span class="math notranslate nohighlight">\(\tilde{f}\)</span> is the composition of the three algorithms above. Now we
ask: “Is this composition of algorithms stable?”</p>
<p>We already know that the Householder algorithm is stable, and a
floating point implementation produces <span class="math notranslate nohighlight">\(\tilde{Q},\tilde{R}\)</span> such that
<span class="math notranslate nohighlight">\(\tilde{Q}\tilde{R}=A+\delta A\)</span> with <span class="math notranslate nohighlight">\(\|\delta
A\|/\|A\|=\mathcal{O}(\varepsilon)\)</span>. It turns out that the implicit
multiplication algorithm is also backwards stable, for similar reasons
(as it is applying the same Householder reflections). This means that
given <span class="math notranslate nohighlight">\(\tilde{Q}\)</span> (we have already perturbed <span class="math notranslate nohighlight">\(Q\)</span> when forming it using
Householder) and <span class="math notranslate nohighlight">\(b\)</span>, the floating point implementation gives
<span class="math notranslate nohighlight">\(\tilde{y}\)</span> which is not exactly equal to <span class="math notranslate nohighlight">\(\tilde{Q}^*b\)</span>, but instead
satisfies</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\tilde{y}= (\tilde{Q}+\delta{Q})^*b \implies
(\tilde{Q} + \delta{Q})\tilde{y} = b,\]</div>
</div></blockquote>
<p>for some perturbation <span class="math notranslate nohighlight">\(\delta Q\)</span> with <span class="math notranslate nohighlight">\(\|\delta
Q\|=\mathcal{O}(\varepsilon)\)</span> (note that <span class="math notranslate nohighlight">\(\|Q\|=1\)</span> because it is
unitary). Note that here, we are treating <span class="math notranslate nohighlight">\(b\)</span> as fixed and considering
the backwards stability under perturbations to <span class="math notranslate nohighlight">\(\tilde{Q}\)</span>.</p>
<p>Finally, it can be shown (see Lecture 17 of Trefethen and Bau for a
proof) that the backward substitution algorithm is backward
stable. This means that given <span class="math notranslate nohighlight">\(\tilde{y}\)</span> and <span class="math notranslate nohighlight">\(\tilde{R}\)</span>, the
floating point implementation of backward substitution produces
<span class="math notranslate nohighlight">\(\tilde{x}\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[(\tilde{R} + \delta \tilde{R})\tilde{x} = \tilde{y},\]</div>
</div></blockquote>
<p>for some upper triangular perturbation such that <span class="math notranslate nohighlight">\(\|\delta
\tilde{R}\|/\|\tilde{R}\|=\mathcal{O}(\varepsilon)\)</span>.</p>
<p>Using the individual backward stability of these three algorithms,
we show the following result.</p>
<div class="proof proof-type-theorem" id="id18">

    <div class="proof-title">
        <span class="proof-type">Theorem 3.16</span>
        
    </div><div class="proof-content">
<p>The QR algorithm to solve <span class="math notranslate nohighlight">\(Ax=b\)</span> is backward stable, producing
a solution <span class="math notranslate nohighlight">\(\tilde{x}\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[(A+\Delta A)\tilde{x} = b,\]</div>
</div></blockquote>
<p>for some <span class="math notranslate nohighlight">\(\|\Delta A\|/\|A\|=\mathcal{O}(\varepsilon)\)</span>.</p>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>From backward stability for the calculation of <span class="math notranslate nohighlight">\(Q^*b\)</span>, we have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}b = (\tilde{Q}+\delta Q)\tilde{y},\\= (\tilde{Q} + \delta Q)(\tilde{R} + \delta R)x,\end{aligned}\end{align} \]</div>
</div></blockquote>
<p>having substituted the backward stability formula for back
substitution in the second line. Multiplying out the brackets
and using backward stability for the Householder method gives</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}b = (\tilde{Q}\tilde{R} + (\delta Q)\tilde{R} + \tilde{Q}\delta R
+ (\delta Q)\delta R)\tilde{x},\\= \underbrace{(A + \delta A + (\delta Q)\tilde{R} +
\tilde{Q}\delta R
  + (\delta Q)\delta R)}_{=\Delta A}\tilde{x}.\end{aligned}\end{align} \]</div>
</div></blockquote>
<p>This defines <span class="math notranslate nohighlight">\(\Delta A\)</span> and it remains to estimate each of these
terms. We immediately have <span class="math notranslate nohighlight">\(\|\delta A\|=\mathcal{O}(\varepsilon)\)</span>
from backward stability of the Householder method.</p>
<p>Next we estimate the second term. Using <span class="math notranslate nohighlight">\(A + \delta A =
\tilde{Q}\tilde{R}\)</span>, we have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\tilde{R} = \tilde{Q}^*(A + \delta A),\]</div>
</div></blockquote>
<p>we have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{R}\|}{\|A\|} \leq \|\tilde{Q}^*\|
\frac{\|A+\delta A\|}{\|A\|} = \mathcal{O}(1), \mbox{ as }
\varepsilon \to 0.\]</div>
</div></blockquote>
<p>Then we have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|(\delta Q)\tilde{R}\|}{\|A\|}
\leq \|\delta Q\|\frac{\|\tilde{R}\|}{\|A\|}
= \mathcal{O}(\varepsilon).\]</div>
</div></blockquote>
<p>To estimate the third term, we have</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{Q}\delta R\|}{\|A\|} \leq \frac{\|\delta
R\|}{\|A\|}\underbrace{\|\tilde{Q}\|}_{=1} =
\underbrace{\frac{\|\delta
R\|}{\|\tilde{R}\|}}_{\mathcal{O}(\varepsilon)}
\underbrace{\frac{\|\tilde{R}\|}{\|A\|}}_{\mathcal{O}(1)}
= \mathcal{O}(\varepsilon).\]</div>
</div></blockquote>
<p>Finally, the fourth term has size</p>
<div class="math notranslate nohighlight">
\[\frac{\|\delta Q\delta R\|}{\|A\|} \leq
\underbrace{\|\delta Q\|}_{\mathcal{O}(\varepsilon)}
\underbrace{\frac{\|\delta R\|}{\|\tilde{R}\|}}_{\mathcal{O}(\varepsilon)}
\underbrace{\frac{\|\tilde{R}\|}
{\|A\|\}}}_{\mathcal{O}(1)} = \mathcal{O}(\epsilon^2),\]</div>
<p>hence <span class="math notranslate nohighlight">\(\|\delta A\|/\|A\|=\mathcal{O}(\varepsilon)\)</span>.</p>
</div></div><div class="proof proof-type-corollary" id="id19">

    <div class="proof-title">
        <span class="proof-type">Corollary 3.17</span>
        
    </div><div class="proof-content">
<p>When solving <span class="math notranslate nohighlight">\(Ax=b\)</span> using the QR factorisation procedure above, the
floating point implementation produces an approximate solution
<span class="math notranslate nohighlight">\(\tilde{x}\)</span> with</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{x}-x\|}{\|{x}\|} = \mathcal{O}(\kappa(A)\varepsilon).\]</div>
</div></blockquote>
</div></div><div class="proof proof-type-proof">

    <div class="proof-title">
        <span class="proof-type">Proof </span>
        
    </div><div class="proof-content">
<p>From <a class="reference internal" href="#accuracy-backward"><span class="std std-numref">Theorem 3.14</span></a>, using the
backward stability that we just derived, we know that</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\|\tilde{x}-x\|}{\|{x}\|} = \mathcal{O}(\kappa\varepsilon),\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\kappa\)</span> is the condition number of the problem of solving
<span class="math notranslate nohighlight">\(Ax=b\)</span>, which we have shown is bounded from above by <span class="math notranslate nohighlight">\(\kappa(A)\)</span>.</p>
</div></div></div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Colin J. Cotter.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>